# 📝 計画とまとめ（進捗記録）

---

## 🔹 フェーズ1：初期モデル構築と誤差分析（2025/4/9）

### 📊 データ基本情報
- 対象変数：SalePrice（住宅価格）
- 平均価格：180,921.20

### 🧪 初期モデル（log変換なし）での検証結果

| モデル             | 学習MSE         | 検証MSE          | Overfit Gap     | 過学習率 |
|------------------|------------------|-------------------|------------------|-------------|
| XGBoost           | 1,664,961        | 722,377,518       | 720,712,558      | 99.77% |
| RandomForest      | 123,345,317      | 839,886,750       | 716,541,433      | 85.31% |
| LinearRegression  | 387,845,561      | 876,261,511       | 488,415,949      | 55.74% |

- MAE（LinearRegression）：  
  - 学習 MAE：12,747.13  
  - 検証 MAE：18,612.20  
  - Overfit Gap Rate：約 31.5%  
  - 平均価格に対する誤差：約 10.3%

### ❗ 問題点
- 高価格な異常値の影響が非常に大きい
- 価格分布が大きく右に偏っている
- MSE の特性により誤差が過大に出やすい

---

## 🔹 フェーズ2：改善施策と再評価（2025/4/10〜）

### ✅ 改善案1：log変換の導入
- `log1p()` によるスケーリング圧縮で、異常値の影響を軽減
- `expm1()` により元の価格に戻して評価

結果（MAE log変換後 → 元スケール）：
```
MAE（log空間）：0.08995  
MAE（実価格）：15,030.64  
```

---

### ✅ 改善案2：モデルチューニング（GridSearchCV）

#### 🔸 RandomForest
```
Best Params: {'max_depth': None, 'min_samples_split': 5, 'n_estimators': 100}
```

#### 🔸 XGBoost
```
Best Params: {
  'colsample_bytree': 0.8,
  'learning_rate': 0.1,
  'max_depth': 3,
  'n_estimators': 200,
  'reg_alpha': 0,
  'reg_lambda': 1,
  'subsample': 0.8
}
```

#### 調整後のモデル比較（log変換 + MSE）

| モデル               | 学習MSE         | 検証MSE         | Overfit Gap     | 過学習率 |
|---------------------|------------------|------------------|------------------|-------------|
| Tuned XGBoost        | 125,043,788      | 634,628,075      | 509,584,287      | 80.30% |
| XGBoost              | 996,435          | 693,102,948      | 692,106,513      | 99.86% |
| RandomForest         | 129,288,537      | 884,971,729      | 755,683,192      | 85.39% |
| Tuned RandomForest   | 170,676,359      | 885,379,048      | 714,702,689      | 80.72% |
| LinearRegression     | 306,741,366      | 524,094,656      | 217,353,290      | 41.47% |

---

## 🔹 フェーズ3：正則化の導入と評価（2025/4/11）

### ✅ Ridge 回帰（L2）
```
Best Alpha: 0.1
MAE（実価格）：15,001.43
MSE（実価格）：518,563,430.67
```

→ Alpha=20.0 にすると MAE と MSE が劣化（過正則化）

### ✅ Lasso 回帰（L1）
- 自動特徴選択の目的で実施
- 残った特徴量：84 / 287

```
Best Alpha: 0.001  
MAE（チューニング後）：178,827.81（大幅悪化）  
MSE：39,649,650,005.96
```

---

## 📚 学習中の気づき・思考の記録（メモ）

- ✅ MAEとMSEの使い分けについて：
  - MAEは「平均的なズレ（全体傾向）」に強く、極端値に弱い。
  - MSEは「極端なズレの影響」を強く受けるので、豪邸などの外れ値の評価に向いている。
  - → **目的が「全体的な安定性重視」ならMAE、「極端なエラー回避」ならMSE**

- ✅ log変換の効果：
  - スケールを圧縮することで、極端値の影響を抑えることができた
  - モデルによっては log変換後のMSE が劇的に改善（特に XGBoost）

- ✅ Lassoによる特徴選択：
  - 有効な特徴量を自動で絞り込めるが、モデル精度が落ちることもある
  - Ridgeとの比較で、Lassoは汎化性能が不安定になりやすいと感じた

- ✅ ハイパーパラメータ調整の重要性：
  - デフォルト設定では明らかに過学習が起きていた
  - GridSearch で調整した結果、バリデーション誤差は明らかに改善

---

## 🏁 結論：最終モデルの決定（2025/4/16）

- ✅ **採用モデル：Ridge（alpha=0.1）**
- MAE：15,001.43
- MSE：518,563,430.67
- 過学習も比較的抑えられており、最も安定した汎化性能を持つ

---

## 🔜 今後のTo-do

1. Ridge モデルを使って **テストデータを予測**（✅ 完了）
   - 出力結果：`result/result.csv`
2. 特徴量重要度の可視化
3. 予測 vs 実際の価格の散布図作成
4. （案）高級物件 vs 一般物件のセグメント建模による再構築

---

📁 モデル保存：
- `model/final_ridge_model.pkl`（最終モデル）
- `model/train_columns.npy`（特徴列順序）

📁 出力結果：
- `result/result.csv`
